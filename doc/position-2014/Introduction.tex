\section{Introduction}

Resource management (RM) software is critical
for high performance computing (HPC).
It is the centerpiece that allows efficient
execution of HPC applications while providing
the computing facility with the main means
to maximize the utilization of its diverse array of computing
resources.

However, several growing trends make even
best-in-breed RM software systems increasingly ineffective
in dealing with the diversity and size of resources fielded for HPC centers.
As numbers and types of compute cores
continue to grow, the key RM challenges traditionally associated only
with leadership-class machines are now
relevant to all computing resources, including
commodity Linux clusters. An effective HPC RM must increase
its purview to manage resources across the entire
computing facility and to enable extreme scalability,
low noise, fault tolerance,
and heterogeneity management delivered within increasingly
stricter power bounds.

In fact, a greater interplay among various classes
of clusters across the entire computing facility already 
makes the current paradigm of single-cluster scheduling
suboptimal. An application running on a compute
cluster heavily utilizes site-wide shared resources
such as I/O and visualization clusters.
Thus, avoiding any significant site-wide bottleneck
requires the RM to schedule the job to all dependent
resources together.

Meanwhile, greater difficulties in code development
on larger systems have begun to impose far more complex
requirements on the RM. For example, without adequate
RM support, debugging, tuning, testing and verification
of the applications have become too difficult
and time-consuming for end-users.
The next-generation code development environments
require the RM to provide effective mechanisms
to support the reproducible results of program execution,
to provide accurate correlations between user-level errors
and system-level events,
and to integrate and accelerate a rich set of scalable tools.

In short, an effective RM that effectively
addresses all of these challenges can net HPC centers
and their users significant productivity gains.

Our response to this critical need is \flux,
a RM software framework that addresses the key emerging
challenges in a simple, extensible, distributed
and autonomous fashion.
It aims at managing the whole computing facility
as one common pool of diverse resources.
Hence, scheduling decisions will be far more efficient
as well as extendible to accommodate emerging constraints
such as power capping.

Further, the \flux\ design will integrate system monitoring and
administration, lightweight virtualization, 
and distributed tool communication capabilities 
that are currently provided by disjoint
and often overlapping software. 
Integration of these facilities within the common framework
designed from the ground up for scalability, security,
and fault tolerance will result in a more efficient
and capable system.

In this paper we introduce our concept of the \flux\ next-generation
resource management framework
and focus on two core elements that we have prototyped: a
communication framework including the Comms Message Broker (CMB), and
workload
run-time tools for efficient execution of transactions within a job. In the
latter category we have developed a distributed Key Value Store (KVS) and
scalable process management services, as well as other components and a
model
using the concept of lightweight jobs (LWJ).
Here we present a detailed description of our CMB and KVS prototypes
as well as some preliminary results and a performance model that
demonstrates scaling properties of these core \flux\ components.
The results have supported and will continue to guide our design
choices as we build additional parts of our RM infrastructure.
\flux\ will enable developers at the operating system and
run-time levels to leverage the RM data stores and services in
unprecedented ways, and it will position HPC centers to cope
with diverse, extreme-scale resources.

\ifcomments
\marginpar{\tiny DA: Please check your author information.}

\ifcomments
\marginpar{\tiny DA: Author list alphabetic for now.}

\ifcomments
\marginpar{\tiny DA: List contributions here?}

\ifcomments
\marginpar{\tiny DA: Paper roadmap here}

