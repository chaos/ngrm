\section{Introduction}

Resource management (RM) software is critical
for high performance computing (HPC).
It is the centerpiece that allows efficient
execution of HPC applications while providing
the computing facility with the main means
to maximize the utilization of its diverse array of computing
resources.
However, several growing trends make even
best-in-breed RM software systems increasingly ineffective
in dealing with the diversity and size of resources fielded for HPC centers.
As numbers and types of compute cores
continue to grow, the key RM challenges traditionally associated only
with leadership-class machines are now
relevant to all computing resources, including
commodity Linux clusters. An effective HPC RM must increase
its purview to manage resources across the entire
computing facility and to enable extreme scalability,
low noise, fault tolerance,
and heterogeneity management delivered within increasingly
stricter power bounds.

A greater interplay among various classes
of clusters across the entire computing facility already 
makes the current paradigm of single-cluster scheduling
suboptimal. An application running on a compute
cluster heavily utilizes site-wide shared resources
such as I/O clusters. 
%and visualization clusters.
Avoiding any significant site-wide bottleneck
requires the RM to schedule the job to all dependent
resources.

Meanwhile, greater difficulties in code development
on larger systems have begun to impose far more complex
requirements on the RM. For example, without adequate
RM support, debugging, tuning, testing, and verification
of applications have become increasingly difficult
and time-consuming for end-users.
Next-generation code development environments
require the RM to provide effective mechanisms
to ease this user burden with features that support the reproducible results of program execution,
provide accurate correlations between user-level errors
and system-level events,
and facilitate integration and accelerate development of a rich set of scalable tools.

In short, an RM that effectively
addresses all of these challenges can net HPC centers
and their users significant productivity gains.
Our response to this critical need is \flux,
an RM software framework that addresses the key emerging
challenges in a simple, extensible, distributed,
and autonomous fashion.
It aims at managing the whole computing facility
as one common pool of diverse resources.
Hence, scheduling decisions will be far more efficient
as well as flexible to accommodate emerging constraints
such as power capping.
Further, the \flux\ design will integrate system monitoring and
administration, lightweight virtualization, 
and distributed tool communication capabilities 
that are currently provided by disjoint
and often overlapping software. 
Integration of these facilities within the common framework
designed from the ground up for scalability, security,
and fault tolerance will result in a more efficient
and capable system.

In this paper we introduce \flux\ and specifically focus on two core 
elements that we have prototyped: a
communication framework including the Comms Message Broker (CMB), and
workload run-time services that enable efficient, interoperable 
execution of transactions within a job. 
CMB represents a novel back-bone overlay network
for \flux,
which can replace all the redundant and independent 
daemon infrastructures that currently exist.  
In the latter category, \flux\ builds its services
around a novel concept called lightweight jobs (LWJs)
and a scalable Key Value Store (KVS) mechanism.

The preliminary results of our performance evaluation 
and our  performance model show that KVS and CMB provides 
strong and predictable scaling properties.
Further, our case study on integrating various key run-time software software
programs indicate that \flux's run-time elements 
provide these programs with 
easy integration and interoperability.
These results continue to guide our design choices
as we build additional parts of our RM framework.


%It is our hope that \flux\ will enable developers at the operating system and
%run-time levels to leverage the RM data stores and services in
%unprecedented ways, and that it will position HPC centers to cope
%with diverse, extreme-scale resources..
