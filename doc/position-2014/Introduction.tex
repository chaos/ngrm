\section{Introduction}

Scalable Resource and Job Management Software (RJMS) is critical
for High Performance Computing (HPC).
It is the centerpiece that enables efficient
execution of HPC applications while providing
the compute center with the means
to maximize the utilization of its diverse  
 computing resources~\cite{GeorgiouThesis}.
However, current state-of-the-art RJMS systems 
are becoming increasingly
ineffective in dealing with the growing diversity and size 
of resources fielded in HPC centers today. Large-scale 
resources are no longer limited to  
%As numbers and types of compute resources
%continue to grow, the key management challenges 
%traditionally associated with only 
a few extremely large individual machines 
like Sequoia (LLNL)~\cite{sequoia}, % or Titan (ORNL)~\cite{titan}, 
%quickly 
but are becoming commonplace through all computing resources sited at a center, including
commodity Linux clusters.
A modern RJMS must provide management services 
to all of the resources at the center,
while maintaining
scalability, low noise, fault tolerance, and enforcing global constraints.
%and heterogeneity management delivered 
%within increasingly stricter power bounds.

In addition, greater difficulties in code development
on larger systems impose increasingly challenging
%requirements on the RJMS, 
requirements on 
debugging, tuning, testing, and verification
%of applications have become too difficult and 
%time-consuming for end-users
%Next-generation code development environments
%require the RJMS to provide effective mechanisms
%to ease this user burden with features that support 
%the reproducible results of program execution,
as well as new techniques for 
%provide 
accurate correlations between user-level errors
and system-level events.
These tools
require adequate RJMS support~\cite{STAT,SPINDLE,PRUNER,SCR,launchmon}
for launching of daemons, allocation of analysis resources, or the ability
for secure third-party access to running jobs.
%,
%and facilitate integration and development 
%of a rich set of scalable tools.
Unless the RJMS can effectively
address these requirements, HPC users 
may suffer significant productivity losses
across the full application development life-cycle. 

The traditional resource and job management paradigm
for large HPC centers organizes the resources sited
at a center in a static and flat hierarchy. Typically, it 
runs an RJMS instance such as SLURM~\cite{Jette02slurm}
to manage and schedule an individual cluster,
and those instances are often connected
together by {\em separate} job management software, 
like MOAB~\cite{MOAB}, PBS Pro~\cite{PSBPro}, and LSF~\cite{LSF}.
While simple, the increasing interplay 
between various classes of compute resources 
across the center renders this paradigm often
inflexible and ineffective. 
For example, this paradigm cannot effectively
schedule applications that utilize site-wide shared 
resources such as file systems. 
Without scheduling file I/O-intensive jobs 
to both compute resources and file systems, 
overlapping I/O bursts coming from only a handful of 
unrelated jobs can disrupt the entire center~\cite{SCR,SPINDLE}. 
In addition, with such an inflexible and shallow hierarchy, 
dynamically imposing complex 
resource constraints at various levels at the center
is becoming increasingly intractable. 
%Avoiding any significant site-wide bottleneck
%requires the RJMS to schedule the job to all dependent
%resources that often span the boundaries 
%of a single cluster.

To address these challenges, we need 
%In this paper, we argue for 
a new management paradigm that is capable of scalably managing 
all of the resources (e.g., compute, storage, and visualization)
at a center together {\bf under one common RJMS framework},
(co-)scheduling jobs to these various types of resources, and 
dynamically enforcing global and local resource constraints. 
% that can
%effectively address the resource and job management 
%challenges that large HPC centers are increasingly facing.
%We argue that the new paradigm 
%must increase its purview 
%to manage resources across the entire center under
%one common RJMS framework, and 
%be able to 
The only way to achieve this with scalability in terms of the total number 
of resources as well as jobs is that the RJMS system must
%Further it mustWe argue that the new paradigm must 
facilitate management and scheduler parallelism~\cite{Omega,Mesos}
and further provide this parallelism through hierarchical, multilevel 
management and scheduling schemes with support for arbitrary numbers of levels.
The latter also requires capabilities for customization or specialization (in terms
of policy, access control, etc) at any level of this dynamic hierarchy 
based on user requests.
%, and 
%be capable of specializing each level.

In this paper, we present \flux, a new open-source RJMS framework that implements 
this new paradigm. We will describe
its design and run-time architecture and show how it will be capable of providing
the necessary RJMS capabilities for next-generation systems and centers. To
demonstrate its feasibility, we have prototyped and evaluated two of its core run-time components:
%
%We have implemented key aspects
%of the \flux 
%
%
%It will be a long journey to fully implement this new paradigm. 
%But we have made a few significant steps to embody the paradigm:
%we started to develop an open-source RJMS framework, \flux,
%and to explore its run-time core components.
%Thus, this paper also presents our early explorations
%of two core run-time elements: 
the communication framework termed the Comms Message Broker (CMB)
and Key Value Store (KVS) to hold \flux state.
%
%
%workload run-time services for efficient, interoperable
%launching and organizing of the distributed processes --- 
CMB and KVS together represent a novel back-bone overlay network for \flux,
which can replace all the redundant and independent
daemon infrastructures that currently exist in a typical cluster.

This paper makes the following contributions:
\begin{itemize}
\item{We describe, based on experience in one of the world's largest HPC centers, the emerging resource and job management challenges that demand a new paradigm;}
\item{We present the design and run-time architecture of \flux, a novel RJMS system that embodies this new paradigm;}
\item{We evaluate two of the core run-time components of \flux to demonstrate the feasibility of our approach.}
\end{itemize}

The results of our performance evaluation 
and performance models show that both KVS and CMB provide 
strong and predictable scaling properties and provide a scalable foundation 
for the \flux architecture.
%Further, our case study on integrating various key run-time software software
%programs indicate that \flux's run-time elements 
%provide these programs with 
%easy integration and interoperability.
%These results continue to guide our design choices
%as we progressively realize our vision. 

