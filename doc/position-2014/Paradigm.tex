\section{A new management paradigm for HPC}
\label{label:paradigm}
A HPC center provides a large array of computing
platforms on which scientific applications perform
computations.
Here, an RJMS system is responsible for efficiently delivering 
compute cycles of these platforms
to multiple users of the applications. 
Thus, the RJMS matches users' jobs with the available 
resources and provides efficient functions 
for building, submitting, launching and executing, 
and monitoring jobs~\cite{GeorgiouThesis}. 
Typically, an HPC center runs an RJMS instance~\cite{Jette02slurm} 
to manage an individual system (e.g., cluster). Often,
grid software ties these instances together. 

However, several growing trends present major challenges 
to the current management paradigm. 
First, most systems sited at the center are growing larger 
and types of resources are becoming increasingly diverse~\cite{GeorgiouThesis}. 
%The RJMS must now provide  
%extreme scalability, low
%noise, fault tolerance, and heterogeneity management
%to most of the resources sited across the entire center. 
Second, with these trends, interplays between 
various classes of resources
(e.g., between compute clusters and a global file system)
are becoming more complicated and disruptive~\cite{SCR,SPINDLE}. 
Third, resource constraints are also becoming increasingly
complex in a multidimensional and hierarchical fashion~\cite{power-overprovision}
(e.g., dynamic power capping at the level of compute racks).
Further, greater difficulties in code development
impose challenging requirements on the RJMS~\cite{STAT,SPINDLE,PRUNER,SCR,launchmon}.
Finally, the workloads themselves are becoming 
diverse, dynamic, and large.

To address the issues stemming from these trends effectively,
large HPC centers demand a new resource and job management paradigm.
The new paradigm must increase its purview
to manage resources across the entire center
{\bf under one common RJMS framework}.
This would allow the center to (co-)schedule jobs 
to various types of resources and 
provide much richer provenance on jobs (e.g., correlation between
a user-level error and other system activities).

Undoubtedly, the new paradigm will face unprecedented scale challenges
in job scheduling. Thus, it must facilitate scheduler
parallelism~\cite{Omega,Mesos} through hierarchical,
multilevel resource management and job scheduling.
In this multilevel scheme, higher-level schedulers
should allow a site to impose site-wide policies, contracts,
and constraints while lower-level schedulers should allow
efficient use of any subsets of resources in accordance with
workload types.
The new paradigm must dynamically support arbitrary levels in
this resource and job hierarchy.

%It is important to note that all of these must 
%be enabled by one common RJMS framework. 
%The current paradigm of loosely coupling different software 
%(i.e., grid software with cluster-wide RJMS instances) 
%leads to a loss of important system knowledge
%needed for effective resource and job management and scheduling. 
%Different software employ distinct resource and
%job representations and key data are often lost in translation.
%Also, the single common framework can offer 
%common run-time commonents as a backbone of
%supporting hierarchical management of resources and jobs
%in a highly scalable, fault-tolerant, secure and customizable
%manner.

We realize that embodying the new paradigm is non-trivial, as
the following characterizes the challenges it must address.

\noindent{\em Multidimensional scale challenge: } 
The new paradigm requires that the RJMS can impose
complex, multidimensional resource bounds at any scale, 
from the center-wide level, down to the level of individual processes,
and enable the most efficient execution and
scheduling of workloads within these bounds.
This imposes unprecedented scale challenges. 
We generally characterize this challenge 
as the {\em multidimensional scale} challenge. The challenges include
supporting extreme scalability, addressing noise as concurrency
increases, and managing a drastically increased amount of
run-time information that must be monitored, traced, and stored.
The new paradigm must efficiently handle increased scale in
numbers of resources as well as jobs and other dimensions 
of RJMS data.

\noindent{\em Diverse workload challenge: }
HPC applications are known to have disparate performance 
limiting factors. This requires
the new paradigm to have a rich resource model.
This includes a representation for diverse
types of resources such as file systems, networks, visualization
hardware, and heterogeneous compute engines, in addition to
traditional clusters.
With a richer resource model, the RJMS will be capable of imposing
complex, multidimensional resource bounds, as opposed to the
simplistic traditional resource model that is fundamentally based on
flat lists of nodes.
With the ability to model sophisticated resource relationships,
the new paradigm will be able to schedule and
allocate resources tailored to the disparate limiting
factors of HPC applications.  

\noindent{\em Dynamic workload challenge: }
A third requirement of the new paradigm is that resource allocations
must be elastic. HPC applications typically have different
phases with disparate performance-limiting factors;
it must be able to grow and shrink its resource allocation
dynamically. Further, different resource types have different
elastic properties 
(e.g., power is a much more elastic resource than compute nodes).
HPC applications and their programming models are
becoming increasingly dynamic with different multi-resource requirements
at different phases. 
The paradigm must support multiple levels of elasticity~\cite{Convergence} 
as a function of dynamically changing performance limiters
as well as their limiting resource types---e.g., 
rigid vs. moldable vs. malleable scheduling 
against different workload and resource types.

\noindent{\em Productivity challenge: }
The new paradigm must address increasing complexities
in code development and system administration by facilitating
the creation of more effective diagnostic and analysis tools.
For example, it must provide basic, scalable monitoring and communication
primitives at the job level that can be leveraged by tools.
It will encourage a richer, stronger tool ecosystem.
% where
%these basic but non-trivial capabilities need not be recreated
%from scratch in every tool, reducing development costs.
Better tools will lead to higher productivity for all
stakeholders, including end users.
%We characterize this design challenge as the {\em productivity challenge}.

\noindent{\em Other challenges: }
We have considered additional challenges. For example,
the risk of higher downtime costs arises in a more global model.
If the RJMS is not designed adequately, a downtime could negatively
impact the availability of a large portion of the center's
resources. Thus, the RJMS must be tolerant of hardware and software
faults and failures with no single point of failure and must
also support live software upgrades. 
Other challenges include security, integration risk, 
and backwards compatibility. 
%will be addressed in our future work.

%
