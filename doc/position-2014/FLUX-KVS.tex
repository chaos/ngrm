\subsection{Distributed Key-Value Store}

We determined that a Key-Value Store (KVS) would be a
useful building block for our new system and prototyped several designs.
Early \flux\ KVS's were based on Redis~\cite{Redis} and Twitter's
twemproxy~\cite{Twemproxy} for sharding and were used mainly to implement
PMI~\cite{PMI2} under MPI.  More recently our KVS has been employed 
in addition for configuration, monitoring, resource descriptions, and
standard I/O streams, and has been rewritten from scratch to meet these
new demands and obtain the most benefit from our persistent CMB overlay
networks.

Our current prototype stores JSON values under a hierarchical key space
with a single master node and multiple caching slaves.  The weak consistency
of our slave caches has the following properties, using the taxonomy
presented in {\em Eventually Consistent}\cite{Vogels:2009:EC:1435417.1435432}:
\begin{itemize}
\item{{\em causal consistency}:  If process A communicates with process B
that it has updated a data item (passing a {\em store version} in that
message), a subsequent access by process B will return the updated value.}
\item{{\em read-your-writes consistency}:  A process having updated a
data item, never accesses an older value.}
\item{{\em monotonic read consistency}:  If a process has seen a particular
value for an object, any subsequent accesses will never return previous values.}
\end{itemize}

We achieve these properties with a simple design based on hash trees
and content-addressable storage, borrowing ideas from ZFS, git, and 
Venti (citations).  JSON objects are placed in a content-addressable
{\em object store}, hashed by their SHA1 digests.
Hierarchical key names are broken up into path components that reference
directories.
A directory is a table of names versus SHA1 object store references,
which is itself placed in the store.  An external root directory SHA1
reference points to the root directory.
For example, if the SHA1 root reference is {\tt 1c002dd...}, and we have
stored {\tt a.b.c = 42} in the KVS, we would look it up as follows:
\begin{enumerate}
\item{load root directory from {\tt 1c002dd...}, find {\tt a} is at
{\tt 3f2243ef...}.}
\item{load {\tt a} from {\tt 3f2243ef...}, find {\tt b} is at
{\tt 023e9b2d...}.}
\item{load {\tt b} from {\tt 023e9b2d...}, find {\tt c} is at
{\tt 7ff234a8...}.}
\item{load {\tt c} from {\tt 7ff234a8...}, and return it (42).}
\end{enumerate}

An important property of this structure is that any update results
in a new SHA1 root reference.  For example if we update {\tt a.b.c = 43}, we:
\begin{enumerate}
\item{store 43 to {\tt 62302aff...}.}
\item{update {\tt b} to associate {\tt c} with {\tt 62302aff...}, and store {\tt b} to {\tt 8fe9b2c3...}.}
\item{update {\tt a} to associate {\tt b} with {\tt 8fe9b2c3...}, and store {\tt a} to {\tt aacc76b4...}.}
\item{update root to associate {\tt a} with {\tt aacc76b4...}, and store root to {\tt 033fbe92...}.}
\item{the new root reference is {\tt 033fbe92}.}
\end{enumerate}

All updates are applied first on the master node at the root of the
CMB tree, which then publishes a new root reference as a CMB event.
Slaves keep consistent with the master by switching their root reference
in response to this event, so that all new lookups must begin at the
new root directory.  Objects missing from the slave object cache during
a lookup are faulted in from their CMB tree parent, recursing up the tree
until the request can be fulfilled.  Unused slave object cache entries are
expired after a period of unuse to save memory.

The CMB event overlay network guarantees ordered delivery, which gives
us monotonic read consistency for free.  We achieve read-your-writes
consistency by returning the new root reference in response to a commit
request and applying it before returning to the caller.  We avoid
racing with the event update and potentially breaking monotonic read
consistency by versioning the root references and never running it
backwards.  We achieve causal consistency by allowing this version number
to be read after an update, and by providing another call to wait for this
root version or greater on another node before accessing the value.

The theoretical performance of our KVS prototype should be influenced by
the following aspects of the design:
\begin{itemize}
\item{Slave caches are arranged hierarchically so that the effect of a
cache miss across a large number of processes is mitigated by the tree
fanout.}
\item{If an object already exists in the object store, storing it again
costs nothing, thus identical values and directories are reduced automatically.}
\item{Updates arriving in rapid succession (within 1 msec currently) are
coalesced into a single update to avoid intermediate object versions and
cache invalidations.}
\item{A {\em collective fence} operation is provided, inspired by PMIv2,
which allows updates from many processes to be coalesced into one.}
\item{The root directory object is sent along with its SHA1 reference when the
new root is published, due to the high probability of needing to fault it in.}
\item{Values amounting to an encoded JSON size less than the size of a base64
representation of its SHA1 digest are stored directly in their directory.}
\end{itemize}
Practical performance results are presented in the results section.

Work on the KVS prototype is ongoing, and our prototype still lacks some
important features that will be needed in the production version,
including the ability to make its contents persistent beyond the
life of a comms session,
tolerance of a fault of the KVS master,
and sharding.
