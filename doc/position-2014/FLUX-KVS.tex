\subsection{Distributed Key-Value Store}

Key-Value Stores (KVS) have become ubiquitous building blocks in large-scale
Internet services but have been underutilized in HPC~\cite{Wang:2013:USE:2503210.2503239}.
We determined that a KVS would be a
essential building block for our new system.
After evaluating several prototypes, our current KVS is designed
to exploit our persistent CMB networks so as to 
support various HPC access patterns such as
MPI bootstrapping through well-known interfaces (e.g., PMI),
process monitoring, standard I/O streams, resource descriptions and
configurations. 

%
%Early \flux KVS's were based on Redis~\cite{Redis} and Twitter's
%twemproxy~\cite{Twemproxy} for sharding and were used mainly to implement
%PMI~\cite{PMI2} under MPI.  More recently our KVS has been employed 
%in addition for configuration, monitoring, resource descriptions, and
%standard I/O streams, and has been rewritten from scratch to meet these
%new demands and obtain the most benefit from our persistent CMB networks.
%
Our current prototype stores JSON values under a hierarchical key space
with a single master node and multiple caching slaves.  The weak consistency
of our slave caches has the following properties, using Vogels'
taxonomy~\cite{Vogels:2009:EC:1435417.1435432}.

\begin{itemize}
\item{{\em Causal consistency}:  If process A communicates with process B
that it has updated a data item (passing a {\em store version} in that
message), a subsequent access by process B will return the updated value.}
\item{{\em Read-your-writes consistency}:  A process having updated a
data item, never accesses an older value.}
\item{{\em Monotonic read consistency}:  If a process has seen a particular
value for an object, any subsequent accesses will never return previous values.}
\end{itemize}

We achieve these properties with a simple design based on hash trees
and content-addressable storage, borrowing ideas from
ZFS~\cite{Bonwick03thezettabyte} and git.%~\cite{Chacon:2009:PG:1618548}.
%, and
%Venti~\cite{Quinlan:2002:VNA:645371.651321}.
JSON objects are placed in a content-addressable
{\em object store}, hashed by their SHA1 digests.
Hierarchical key names are broken up into path components that reference
directories.
A directory is an object that maps a list of names to other objects by
their SHA1 reference.
An external root directory SHA1 reference points to the root directory object.
For example, if the SHA1 root reference is {\tt 1c002dde...}, and we have
stored {\tt a.b.c = 42}, we would look it up as follows:
%(refer to Figure~\ref{fig:kvsupdate1}):
\begin{enumerate}
\item{load root directory from {\tt 1c002dde...}, find {\tt a} is at
{\tt 3f2243ef...}.}
\item{load {\tt a} from {\tt 3f2243ef...}, find {\tt b} is at
{\tt 023e9b2d...}.}
\item{load {\tt b} from {\tt 023e9b2d...}, find {\tt c} is at
{\tt 7ff234a8...}.}
\item{load {\tt c} from {\tt 7ff234a8...}, and return it (42).}
\end{enumerate}

An important property of this structure is that any update results
in a new SHA1 root reference.  Continuing the example, to update {\tt a.b.c = 43}, we:
\begin{enumerate}
\item{store 43 to {\tt 62302aff...}.}
\item{update {\tt b} to associate {\tt c} with {\tt 62302aff...}, and store {\tt b} to {\tt 8fe9b2c3...}.}
\item{update {\tt a} to associate {\tt b} with {\tt 8fe9b2c3...}, and store {\tt a} to {\tt aacc76b4...}.}
\item{update root to associate {\tt a} with {\tt aacc76b4...}, and store root to {\tt 033fbe92...}.}
\item{the new root reference is {\tt 033fbe92}.}
\end{enumerate}


All updates are applied first on the master node at the root of the
CMB tree, which then publishes a new root reference as a CMB event.
Slaves keep consistent with the master by switching their root reference
in response to this event, so that all new look-ups must begin at the
new root directory.  Objects missing from the slave object cache during
a look-up are faulted in from their CMB-tree parent, recursing up the tree
until the request can be fulfilled.  Unused slave object cache entries are
expired after a period of disuse to save memory.

The CMB event overlay network guarantees ordered delivery, which gives
us monotonic read consistency for free.  We achieve read-your-writes
consistency by returning the new root reference in response to a commit
request and applying it before returning to the caller.  We avoid
racing with the event update and potentially breaking monotonic read
consistency by versioning the root references and ensuring they are
never applied out of order.
We achieve causal consistency by allowing this version number
to be read after an update, and by providing another call to wait for this
root version or greater on another node before accessing the value.

The KVS API includes classes of functions for putting, committing, and
getting KVS objects.  
%We refer to these classes as
%{\em producer}, {\em synchronization}, and {\em consumer} respectively
%in the results section.
First, {\tt kvs\_put (key, val)}
writes {\em val} to the object store asynchronously in a write-back
mode through the tree of slave caches.
The ({\em key, SHA1}) tuple is cached locally pending commit.

{\tt kvs\_commit ()} synchronously flushes ({\em key, SHA1}) tuples
and any still-dirty objects to the master.  On the master, it then
processes the set of tuples, creating new directory objects as described
above, finally arriving at a new root SHA1.  It then updates the 
root reference session-wide with a multicast event.
Since both new and old objects coexist in the caches, the switch from old
to new root is atomic.
{\tt kvs\_get\_version ()} and {\tt kvs\_wait\_version ()} are available
for causal consistency as described above.
{\tt kvs\_fence ()} commits for a group of processes collectively
through the internal use of a collective barrier.

%and slight modification to the commit logic on the master.

%\begin{figure}[ht]
%%\vspace{-.5cm}
%\centering
%\begin{subfigure}[a.b.c = 42.]{
%  \fbox{\includegraphics[width=.152\linewidth]{kvs_update_1}}
%  \label{fig:kvsupdate1}
%}%
%\end{subfigure}\hfill
%\begin{subfigure}[a.b.c = 43 in progress.]{
%  \fbox{\includegraphics[width=.34\linewidth]{kvs_update_2}}
%  \label{fig:kvsupdate2}
%}%
%\end{subfigure}\hfill
%\begin{subfigure}[a.b.c = 43 committed.]{
%  \fbox{\includegraphics[width=.34\linewidth]{kvs_update_3}}
%  \label{fig:kvsupdate3}
%}%
%\end{subfigure}\hfill
%\caption{KVS value update scheme}
%\vspace{-.5cm}
%%\caption{Updating a value stored in the \flux\ KVS requires all of its
%%parent directories to be updated.  The transaction is completed
%%(atomically) when the root reference is updated.}
%\label{fig:kvsupdate}
%\end{figure}
%
{\tt kvs\_get (key)} recursively looks up the key starting with the
current root reference, faults in any missing objects
through the tree of slave caches, and returns the terminal object.
{\tt kvs\_watch (key, callback)} is a {\tt get} variant which registers a
callback to be triggered whenever the value of {\em key} changes.
It accomplishes this by internally performing a {\tt get} on the watched
value in response to each root update, comparing the new
and old values, and calling the callback if they are different.
Due to our hash-tree organization, a watched directory changes
if keys under it at any path depth change.

%The theoretical performance of our KVS prototype is influenced by
%several aspects of the design. First,  
%slave caches are arranged hierarchically so that the effect of a
%cache miss across a large number of processes is mitigated by the tree
%fanout. Second, the act of storing an object redundantly in the object store
%is squashed at the first node in the CMB tree that has the object
%in cache, thus identical values and directories are reduced automatically.
%Third, commits are rate-limited.  Updates arriving within a configurable
%period of time (default 1 msec) are coalesced into a single update to
%avoid excessive intermediate object versions and cache invalidations.
%Fourth, the root directory object is sent along with its SHA1 reference when the
%new root is published, due to the high probability of needing to fault it in.
%Fifth, objects with an encoded JSON size less than or equal to the size
%of their base64 SHA1 digest (42 bytes) are stored by value in the directory
%object. Finally, objects are expired from slave caches after not being accessed for
%a period. %configurable period (default 7.5 seconds).
%
%Practical performance results are presented in the results section.
%
%Work on the KVS prototype is ongoing, and our prototype still lacks some
%important features that will be needed in the production version,
%including the ability to make its contents persistent beyond the
%life of a comms session,
%tolerance of a fault of the KVS master,
%and sharding.
