\subsection{Experimental Setup}
We run all of our experiments on two Linux clusters installed at LLNL,
named Zin and Cab.
Each compute node of these clusters has 2 sockets and 32 GB of RAM.
Each socket is populated with an 8-core 2.6 GHz Intel
Xeon E5-2670 processor, resulting in 16 cores per node.
Zin consists of 2,916 compute nodes totaling 46,656 cores;
Cab is a smaller system with the same node type,
consisting of 1,296 compute nodes with a total of 20,736 cores.
Nodes are connected by a Qlogic Infiniband QDR interconnect.
The largest allocation allowed in normal batch mode
is 258 nodes on Cab and 512 on Zin.

We run KAP with varying arguments to its parameters
in batch mode and collect various 
performance metrics. 
Because the exploration
space is huge, however, we limit our experiments with
only a subset of the parameter set and of sampling points.

Specifically, we run our KAP tests at 64, 128, 256 and 512
compute nodes, and always fully populate each node with
16 processes, each acting as consumer or producer or
both. We vary the consumer or producer count
while fixing the other at the total number of cores.
We also vary the value size 
from 8 Bytes to 32 Kilobytes in the powers of 8,
and the key-value object access count of each consumer
from 1 to the total process count.

Further, we evaluate the performance impact 
of how key-value objects are organized 
in KVS by either storing all of the objects into a single KVS directory
or distributing them into multiple directories---i.e.,
128 objects per KVS directory.
Finally, we study the performance implications of 
redundancy in values by either configuring producers to generate
unique or redundant values across them.

For simplicity, we configure the topology of CMB only as 
the binary tree and and use \flux's collective fence 
as our only mechanism to enforce consistency.
