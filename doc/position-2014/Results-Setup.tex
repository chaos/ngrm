\subsection{Experimental Setup}
We ran all of our experiments on two Linux clusters installed at 
Lawrence Livermore National Laboratory (LLNL),
named Zin and Cab.
Each compute node of these clusters has 2 sockets and 32 GB of RAM.
Each socket is populated with an 8-core 2.6 GHz Intel
Xeon E5-2670 processor, resulting in 16 cores per node.
%Zin consists of 2,916 compute nodes totaling 46,656 cores;
%Cab is a smaller system with the same node type,
%consisting of 1,296 compute nodes with a total of 20,736 cores.
Nodes are connected by a Qlogic Infiniband QDR interconnect.
The largest allocation our batch system allows for our tests is
512 nodes (8,192 cores).

We ran KAP with varying arguments to its parameters
in batch mode and collected 
%various 
performance metrics. 
Due to the huge parameter space, however, we limited our experiments to
only a subset of the parameter set.

Specifically, we ran our KAP tests at 64, 128, 256 and 512
compute nodes, and always fully populated each node with
16 processes, each acting as consumer or producer or
both. We varied the consumer or producer count
while fixing the other at the total number of cores.
We also tested the value size at
8, 32, 128, 512, 2048, 8192, and 32,768 bytes,
and the key-value object access count of each consumer
from 1 to the total process count.

Further, we evaluated the performance impact 
of how key-value objects are organized 
in KVS by either storing all of the objects into a single KVS directory
or distributing them into multiple directories of at most 128 objects each.
Finally, we studied the performance implications of 
redundancy in values by either configuring producers to generate
unique or redundant values across them.
For simplicity, we fixed the comms session topology as a binary tree
and used only {\tt kvs\_fence} for synchronization.
