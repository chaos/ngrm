\subsection{Performance Results and Analysis}
\label{results}
Of tens of thousands of our sampling runs, we find that the fully populated
cases---i.e. both producer and consumer counts become equal to the total
process count---are most revealing. In particular, we carefully analyze 
the maximum latency of each of the main phases of KAP for these cases 
because this metric represents the critical path of the performance of
many HPC services. For example, distributed 
HPC software would use KVS operations in a coordinated fashion to exchange 
connection information among processes during its bootstrapping 
phase~\cite{LIBI,PMI2}. Unless {\em all} 
of the distributed processes complete their
KVS operations, their communication fabric cannot be established. 

\begin{figure}
  \centering
  \includegraphics[width=.75\linewidth]{producer}
  \caption{Max latency of producer phase}
  \vspace{-.5cm}	
  \label{fig:prod}
\end{figure}

Figure~\ref{fig:prod} shows the maximum latency of the producer phase
for these cases. Essentially, these plots indicate how well {\tt kvs\_put}
scales as we increase the number of producers. Each plot represents
different value sizes---e.g., vsize-8 refers to value size being
8 bytes. As shown in this graph, the {\tt kvs\_put} simply performs and
scales well. This matches our expectations because objects
are cached in write-back mode at {\tt kvs\_put} time and flushed to the
server at the next consistency event. 

\ifcomments
\marginpar{\tiny DA: I will add data from 8K if I get it on Zin in time.}
\marginpar{\tiny JG: The vsize-32768 requires some explanation}
\fi

\begin{figure}[ht]
\centering
\begin{subfigure}[With unique values]{
  \includegraphics[width=.75\linewidth]{sync}
  \label{fig:sync:noredund}
}%
\end{subfigure}
\begin{subfigure}[With redundant values]{
  \includegraphics[width=.75\linewidth]{sync}
  \label{fig:sync:redund}
}%
\end{subfigure}
\caption{Max latency of synchronization phase}
\label{fig:sync}
\end{figure}


Moving to the synchronization phase, Fig.~\ref{fig:sync} shows 
how {\tt kvs\_fence} scales during this phase
as we increase the number of producers. 
As with the producer latency,
each plot represents different value sizes.
The most revealing observation is that
fence scalabilty appears to depend on the level of
redundancy in key-value objects that had previously 
been put in. Figure~\ref{fig:sync:redund} 
shows the maximum latency of the synchronization phase
when redundant values are used. It scales
far better than Fig.~\ref{fig:sync:noredund}
where there is zero redundancy in values. 
We theorize that in the former case, fence performs linearly with respect to the number of
producers because these unique values are being {\em concatenated} while
being sent up the tree. It performs logarithmically for the latter case
because redundant values are being {\em reduced} while being sent 
up the tree. %%(MENSION R^2 here?)

\ifcomments
\marginpar{\tiny DA: I am speculating a bit for redundant case; my job on cab will confirm or deny this.}
\marginpar{\tiny JG: Dong I can't see any difference between
\ref{fig:sync:redund} and \ref{fig:sync:noredund}.}
\fi

\begin{figure}[ht]
\centering
\begin{subfigure}[With single-directory layout]{
  \includegraphics[width=.75\linewidth]{consumer-1-dir}
  \label{fig:cons:dir}
}%
\end{subfigure}
\begin{subfigure}[Improvements with multiple directories]{
  \includegraphics[width=.75\linewidth]{consumer-dist-dir}
  \label{fig:cons:dirs}
}%
\end{subfigure}
\caption{Max latency of consumer phase (value size: 8 bytes)}
\vspace{-.5cm}
\label{fig:consumer}
\end{figure}

Figure~\ref{fig:consumer} shows the maximum latency of the consumer
phase with respect to various parameter settings. 
These results suggest how well {\tt kvs\_get}
scales, as we increase the number of consumers. Each plot represents
the latency when each consumer reads different numbers of 
objects, e.g., the access-4 plot represents each consumer reading
4 distinct objects.  While these figures show only the performance of reading
objects 8 bytes in size, we observe that the general 
scalability trends are similar at different value sizes.
Suprisingly, we see little change in performance when value sizes
exceed the 42-byte store-by-reference limit.

Figure~\ref{fig:cons:dir} shows the maximum latency of {\tt kvs\_get}
when the target key-value objects are all stored in a single
KVS directory. The latency is quite high and also increases
linearly as we increase the number of consumers. 
It appears that the poor performance and scaling behavior 
can be attributed to the fact that our slave caches store only full
objects, and the small objects being consumed in the test are all
placed in a single large directory object.
This directory object has to be retrieved in full by the slave cache,
through the tree of CMB tree parents, when the first
member object is requested by {\tt kvs\_get} and a fault occurs.

With our access pattern where $G$ objects are read collectively by
$C$ consumers, and the time to replicate G objects in a single slave cache
is given by $T(G)$, the maximum consumer latency is given by
\begin{equation}
log_2(C) * T(G).
\end{equation}
Thus, the max latency increase for every doubling of consumers is
\begin{equation}
\frac{log_2(2C) * T(2G)}{log_2(C) * T(G)}.
\end{equation}
Generally, this would approach 2, 
as we continue to increase the number of consumers,
and our data match with this model.

We can improve this behavior by storing objects across multiple
directories. Slave caches can then operate with a finer granularity,
and the quantity of data that must be retrieved to satisfy consumer
requests will tend, depending on how requests stride directories,
to be proportional to the quantity of data requested.
This is especially true toward the leaves of the CMB tree where the
slave caches service a decreasing subset of consumers.
Figure~\ref{fig:cons:dirs} shows the improvements 
when we spread objects into directories of 128 objects each.
Label {\tt mdir-acc-k} refers to the same access pattern as {\tt access-k} 
except the accessing objects are stored across multiple directories.

\ifcomments
\marginpar{\tiny JG: Dong, can \ref{fig:cons:dir} and \ref{fig:cons:dirs}
be refactored for clarity so the first has access plots and the second
has mdirs-acc plots?}
\fi
When objects are spread across directories and the amount of data
faulted into slave caches decreases as a function of tree levels, 
the latency can be modeled as a geometric series.
For example, at tree height $h$, the latency would be 
\begin{equation}
T(G) + T(\frac{G}{2}) + T(\frac{G}{4}) + ... + T(\frac{G}{2^h}), 
\end{equation}
where each term represents the latency of replication per level,
and the sum approaches $2T(G)$. Thus, its improvement over the single
directory scheme is roughly speaking
\begin{equation}
\frac{log_2(C) * T(G)}{2 T(G)} = \frac{1}{2}log_2(C).
\end{equation}
The improvements would be linearly 
greater as we increase the scale and our measurements agree with this. 

While these results suggest a promising avenue for reworking the KVS internal
object layout for performance, we note that such a scheme alone
is likely to fall short of reaching extreme scale.
Our model tells us that the latency will grow linearly when $G$ grows
with the scale.   For example, if $G$ doubles every time the number of
consumers is doubled, our geometric series model predicts the latency will
also roughly double according to
\begin{equation}
\frac{2T(2G)}{2T(G)}.
\end{equation}
With the current scheme, the only way to gain true logarithmic
scaling is when $G$ stays constant regardless of scale. 
In a later section, we will discuss some of our plans 
to exploit the understanding we gain from the evaluation. 
