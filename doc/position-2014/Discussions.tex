\section{Future Work}

We have validated some aspects of the design of \flux\ CMB and KVS
through the KVS performance evaulation in Section~\ref{results}
and the exercise of prototyping LWJ management primitives for the
case study in Section~\ref{case}.  However, we have discovered some
deficiencies, and have simply not yet tackled some features
that must be addressed before we can build production
quality versions of the CMB and KVS.
These include network security, comprehensive fault tolerance,
KVS persistance, and KVS clustering.

The original \flux\ design described a job and hence a comms session
as having an owner with the ability to control what other users could
execute within the job, including the ability to launch new jobs and
launch LWJ's within the current job.  In addition, the job would launch
work in lightweight, virtualized containers that prevented LWJs from
exceeding their resource allocation.  Finally, a job could span any set
of resources across a data center, meaning comms session overlays must
be capable of spanning multiple, possibly disjoint IP networks.
These features will require the \flux\ communications framework to
support message intergrity and privacy so that users cannot interfere
with one another or attain inappropriate privileges, and to integrate with
enterprise authentication services such as Kerberos.

The \flux\ CMB detects non-responsive nodes, evicts them from
the session, and alters its topology to work around them.
There is not a comprehensive strategy for ensuring that all in-flight
RPC's complete or are restarted across one of these fail-over events,
and that state from the evicted node is recoverable.

The current KVS object cache is stored in memory.
On slave caches, objects are expired after a period of disuse.
On the master, however, objects must remain in memory forever.
Over time, the cache will become filled with old values that may never
be used again.  One solution to this problem is to add a level of
on-disk cache behind the master, and then temporally expire the master's
cache in a manner similar to the slave caches.  This would allow data from
the KVS belonging to a job to be {\em reaped} at job termination and
incorporated in some reduced form into the KVS of the parent job.

Interestingly, if KVS persistence were implemented using an on-disk
object directory structured like the git's~\cite{Chacon:2009:PG:1618548},
which uses SHA1 digests as file names, it could be shared across multiple
masters using a network file system like NFS with close-to-open cache
coherency, since file names will always refer to the same content and
published references could be guaranteed coherent.
Such a scheme could be used to build a clustered KVS master with each
node in the cluster servicing disjoint shards of the key space but
sharing a common object store.
