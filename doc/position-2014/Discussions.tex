\section{Conclusion}
Large HPC centers are increasingly facing 
multifaceted resource management challenges
that if not properly met, will result 
in significant losses.  
\flux\ is our response to these challenges
so as to improve operational efficiency 
and user productivity by changing 
the fundamental way in which large
centers are managed.
Our run-time infrastructure such as CMB and KVS
is a significant first step to this rather long
journey.

We have validated key aspects of these
components to guide our future efforts systematically. 
Our study suggests that our run-time 
infrastructure is most scalable when information 
exchange patterns themselves are also scalable.
And this suggests two significant directions. 
For one, we must carefully design the data exchange patterns
among distributed components of run-time elements. 
To achieve extreme scalability, each component 
must avoid accessing a global view.
Secondly, we must also continue to push the 
scalability envelope of our infrastructure. 
For the latter, we will soon investigate a way to 
shard KVS masters into multiple nodes
and to distribute access patterns to further improve scalability.

Besides scalability, we must also address KVS persistency.
Specifically, we plan to explore a scheme
thereby putting on-disk cache behind the KVS master,
and then temporally expire the master's cache.
In fact, we will investigate ways to link
this scheme to the sharding scheme such a way
that a single on-disk cache can service the value
objects to all of the KVS masters.

There are other deficiencies we must address as well.
These include network security and
comprehensive fault tolerance. 
As we continue to implement these building blocks 
to further our framework, we will be evaluating 
whether the framework can meet our goal for \flux's 
flexibility, scalability, and extensibility. 
Ultimately, it is our hope that \flux\ will 
enable developers at the operating system and
run-time levels to leverage the RM data stores and services in
unprecedented ways, and that it will position HPC centers to cope
with diverse, extreme-scale resources.
