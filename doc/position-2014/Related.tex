\section{Related Work}
\flux seeks to change the paradigm in which 
large HPC centers should manage, model, schedule, and
allocate resources. 
A strong body of research exists in each of these areas 
including production solutions such as SLURM~\cite{Jette02slurm}, 
LSF~\cite{LSF}, Moab~\cite{MOAB}, 
PBS Pro~\cite{PSBPro}, LoadLeveler~\cite{LL}
and Condor~\cite{Litzkow88}.
\flux is distinguished from these approaches, as it provides
a new paradigm that can address emerging resource and
job management challenges 
with a center-wide purview under one common 
software framework. 

In other areas, like cloud and grid computing, the need 
for exploiting scheduler parallelism including two-level 
scheduling recently emerged. 
Google's Omega~\cite{Omega} exploits 
a parallel scheduler architecture whereby multiple
schedulers concurrently access the shared resource state
with an optimistic concurrency model. Efforts
such as Apache Mesos~\cite{Mesos} and many emerging
grid schedulers~\cite{MultilevelGrid,Oar} 
take advantage of two-level scheduling strategies.
However, these approaches are neither optimized for HPC workloads 
nor well suited for large HPC centers. 
Scales of large HPC centers like those at national laboratories
demand a deeper and more dynamic levels in the resource management 
and scheduler hierarchy with an ability to impose constraints 
at various levels in this hierarchy.

HPC trends increasingly motivate scalable KVS implementations. 
Wang et al. proposed a distributed KVS 
as the basis for HPC services to encapsulate
complexity of distributed services~\cite{Wang:2013:USE:2503210.2503239}.
%
%, thereby simplifying the
%tools and services~\cite{Wang:2013:USE:2503210.2503239}.
They further evaluate replacing the centralized controller in
SLURM~\cite{Jette02slurm} with a distributed controller~\cite{Slurmpp}
built on ZHT~\cite{Li:2013:ZLR:2510661.2511401}.
While existing KVS work takes an incremental approach of improving 
scalability of a traditional RJMS paradigm with new scalable services,
ours are built specifically to support the new paradigm. 
We also evaluated Redis~\cite{Redis} and twemproxy~\cite{Twemproxy}
as part of our early KVS investigation.
%In particular, Redis cluster~\cite{RedisClusterTut,RedisClusterSpec} 
%includes many desired properties such as 
%sharding, re-sharding, replication, and failover.
However, their design points are not optimized
for HPC workloads which often feature synchrony and coordination. 
%Our consistency model and mechanisms such as the collective fence 
%specifically optimized for HPC.
%

%There is no proxy; clients connect directly to the server
%for a particular shard.  Redis-cluster's asynchronous replication
%results in the possibility of losing writes that occur during failover.
%
Finally, much research exists in the area of tree-based overlay network (TBON). 
They include MRNet~\cite{mrnet} and COBO~\cite{launchmon}, and 
%In particular, MRNet is a reusable designed to be embedded 
%in large scale HPC tools such as STAT~\cite{STAT}. 
%%They have explored
%{\em state compensation}~\cite{conf/ipps/ArnoldM10}
%as a mechanism for fault tolerance in TBONs that perform data aggregation.
CMB can be considered to be a TBON. But unlike user-level
TBONs, ours must support system-level activities, and this 
requires us to address distinct research topics
such as support for multiple user-level networks (which actually
include other user-level overlay networks), security, low noise 
and fault tolerance. 

