\section{Related Work}
\flux seeks to change the paradigm in which 
how large HPC centers should manage, model, schedule, and
allocate resources. 
A strong body of research exists in each of these areas 
including production solutions such as SLURM~\cite{Jette02slurm}, 
LSF~\cite{LSF}, Moab~\cite{MOAB}, 
PBS Pro~\cite{PSBPro}, LoadLeveler~\cite{LL}
and Condor~\cite{Litzkow88}.
To the best of our knowledge, \flux is a novel open-source 
effort that tackles emerging HPC resource management challenges 
by raising the RJMS's purview to the entire center 
under one common software framework. 

In other areas such as cloud or grid computing, the need 
for exploting scheduler parallelism including two-level 
scheduling also recently emerged. 
Google's Omega~\cite{Omega} exploits 
a parallel scheduler architecture whereby multiple
schedulers concurrently access the shared resource state
with an optimistic concurrency model. Efforts
such as Apache Mesos~\cite{Mesos} and many emerging
grid schedulers~\cite{MultilevelGrid,Oar} 
take advantage of two-level scheduling strategies.
However, these approaches are neither optimized for HPC workloads 
and nor well suited for large HPC centers. 
Scales of a large HPC center demand a deeper 
and dynamic resource management and scheduler hierachy 
with an ability to impose various policies and
constraints at various levels in this hierachy.

HPC trends increasingly motivate scalable KVS implementations 
like ours. Wang et al. proposed a distributed KVS 
as the basis for HPC tools and services to encapsulate
complexity of distributed services~\cite{Wang:2013:USE:2503210.2503239}.
%
%, thereby simplifying the
%tools and services~\cite{Wang:2013:USE:2503210.2503239}.
They further evaluate replacing the centralized controller in
Slurm~\cite{Jette02slurm} with a distributed controller~\cite{Slurmpp}
built on ZHT~\cite{Li:2013:ZLR:2510661.2511401}.
While existing KVS work takes an incremental approach of improving 
scalability of a traditional RJMS paradigm with new scalable services,
ours are built specifically to support the new paradigm. 

We also evaluated Redis~\cite{Redis} and twemproxy~\cite{Twemproxy}
as part of our early KVS investigations.
%In particular, Redis cluster~\cite{RedisClusterTut,RedisClusterSpec} 
%includes many desired properties such as 
%sharding, re-sharding, replication, and failover.
%However, their design points are not optimized
for HPC workloads which often feature synchrony and coordination. 
Our consistency model and mechanisms such as the collective fence 
specifically optimized for HPC.


%There is no proxy; clients connect directly to the server
%for a particular shard.  Redis-cluster's asynchronous replication
%results in the possibility of losing writes that occur during failover.
%
Finally, much research exists in the area of tree-based overlay network (TBON). 
They include MRNet~\cite{mrnet} and COBO~\cite{launchmon}, and 
%In particular, MRNet is a reusable designed to be embedded 
%in large scale HPC tools such as STAT~\cite{STAT}. 
%%They have explored
%{\em state compensation}~\cite{conf/ipps/ArnoldM10}
%as a mechanism for fault tolerance in TBONs that perform data aggregation.
CMB can be considered to be a TBON. But unlike user-level
TBONs, ours must support system-level activities, and this 
requires us to answer distinct research topics
such as support for multiple user-level networks (which actually
include other user-level overlay networks), security, low noise 
and fault tolerance. 

